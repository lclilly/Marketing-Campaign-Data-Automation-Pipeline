# ğŸ“Š Marketing Campaign Data Automation Pipeline

## ğŸ¯ The Problem
Marketing teams were spending **4 hours every week** manually cleaning campaign data:
- Mixed date formats from different team members
- Duplicate entries from multiple uploads
- Missing values and data quality issues
- Manual calculation of KPIs (ROAS, CPA, CTR)

## ğŸ’¡ My Solution
Built an **automated Python pipeline** that:
- âœ… Cleans and standardizes messy marketing data
- âœ… Detects and flags anomalies automatically
- âœ… Calculates 6 key marketing metrics (CTR, CPC, CPA, ROAS, Conversion Rate, Profit)
- âœ… Generates actionable business insights
- âœ… Reduces 4 hours of manual work to 5 minutes

## ğŸ“ˆ Business Impact
- **Time Saved:** 235 minutes per week = 208 hours annually
- **Cost Savings:** ~$10,400 per year (at $50/hour labor cost)
- **Data Quality:** Improved from 65% to 92% "Good Quality" records
- **Insights Generated:** 4 automated recommendations per run

## ğŸ› ï¸ Technical Implementation

### Technologies Used:
- **Python** (pandas, numpy)
- **Data Cleaning:** Standardized 175+ date formats, removed 20 duplicates
- **Anomaly Detection:** IQR method for outliers, business logic validation
- **Metric Calculation:** Automated KPI computation

### Key Features:
1. **Duplicate Detection** - Removes exact duplicates based on campaign identity
2. **Date Standardization** - Handles 6+ different date formats
3. **Anomaly Flagging** - Detects impossible metrics (clicks > impressions)
4. **Quality Scoring** - Assigns 0-100 score to each record
5. **Business Insights** - Generates channel recommendations automatically

## ğŸ“Š Insights Generated:
1. ğŸ“Š Facebook has the highest ROAS at 23.18x - consider shifting more budget here

2. âš ï¸ 17 campaigns have ROAS below 1.0x - potential savings of $56,834.59 by pausing them

3. ğŸ“… Saturday shows the best average ROAS - schedule high-value campaigns on this day

4. ğŸ¯ 'Holiday Promo' has the lowest CPA at $0.38 - model other campaigns after this one

## ğŸ“¸ Results

[SCREENSHOTS ATTACHED]
- Before: Messy data
- After: Clean data
- Dashboard: Performance visualization

## ğŸš€ How to Run
```bash
python marketing_automation_complete.py
```

## ğŸ“ Project Structure
```
â”œâ”€â”€ marketing_automation_complete.py  # Main automation script
â”œâ”€â”€ messy_marketing_data.csv         # Sample input data
â”œâ”€â”€ clean_marketing_data.csv         # Cleaned output
â””â”€â”€ config.json                      # Configuration
```

## ğŸ“ Key Learnings
- Importance of data quality in decision-making
- Trade-offs in duplicate detection logic
- Business context matters in technical decisions
- Automation ROI calculation and communication

What's your biggest data quality challenge? Drop a comment! ğŸ‘‡
